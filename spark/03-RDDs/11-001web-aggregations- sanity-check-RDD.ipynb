{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Creative Commons License\n",
    "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.\n",
    "\n",
    "Credit to DataBricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext\n",
    "import sys\n",
    "import os\n",
    "from operator import add\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick test of the regular expression library\n",
    "# https://www.regextester.com/95830\n",
    "# Look behind 'abc'\n",
    "m = re.search('(?<=abc)def', 'abcdef')\n",
    "m.group(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 1: Apache Web Server Log file format**\n",
    "#### The log files that we use for this assignment are in the [Apache Common Log Format (CLF)](http://httpd.apache.org/docs/1.3/logs.html#common). The log file entries produced in CLF will look something like this:\n",
    "`127.0.0.1 - - [01/Aug/1995:00:00:01 -0400] \"GET /images/launch-logo.gif HTTP/1.0\" 200 1839`\n",
    " \n",
    "#### Each part of this log entry is described below.\n",
    "* `127.0.0.1`\n",
    "#### This is the IP address (or host name, if available) of the client (remote host) which made the request to the server.\n",
    " \n",
    "* `-`\n",
    "#### The \"hyphen\" in the output indicates that the requested piece of information (user identity from remote machine) is not available.\n",
    " \n",
    "* `-`\n",
    "#### The \"hyphen\" in the output indicates that the requested piece of information (user identity from local logon) is not available.\n",
    " \n",
    "* `[01/Aug/1995:00:00:01 -0400]`\n",
    "#### The time that the server finished processing the request. The format is:\n",
    "`[day/month/year:hour:minute:second timezone]`\n",
    "  * ####day = 2 digits\n",
    "  * ####month = 3 letters\n",
    "  * ####year = 4 digits\n",
    "  * ####hour = 2 digits\n",
    "  * ####minute = 2 digits\n",
    "  * ####second = 2 digits\n",
    "  * ####zone = (\\+ | \\-) 4 digits\n",
    " \n",
    "* `\"GET /images/launch-logo.gif HTTP/1.0\"`\n",
    "#### This is the first line of the request string from the client. It consists of a three components: the request method (e.g., `GET`, `POST`, etc.), the endpoint (a [Uniform Resource Identifier](http://en.wikipedia.org/wiki/Uniform_resource_identifier)), and the client protocol version.\n",
    " \n",
    "* `200`\n",
    "#### This is the status code that the server sends back to the client. This information is very valuable, because it reveals whether the request resulted in a successful response (codes beginning in 2), a redirection (codes beginning in 3), an error caused by the client (codes beginning in 4), or an error in the server (codes beginning in 5). The full list of possible status codes can be found in the HTTP specification ([RFC 2616](https://www.ietf.org/rfc/rfc2616.txt) section 10).\n",
    " \n",
    "* `1839`\n",
    "#### The last entry indicates the size of the object returned to the client, not including the response headers. If no content was returned to the client, this value will be \"-\" (or sometimes 0).\n",
    " \n",
    "#### Note that log files contain information supplied directly by the client, without escaping. Therefore, it is possible for malicious clients to insert control-characters in the log files, *so care must be taken in dealing with raw logs.*\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_map = {'Jan': 1, 'Feb': 2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7,\n",
    "    'Aug':8,  'Sep': 9, 'Oct':10, 'Nov': 11, 'Dec': 12}\n",
    "\n",
    "\n",
    "def parse_apache_time(s):\n",
    "    \"\"\" Convert Apache time format into a Python datetime object\n",
    "    Args:\n",
    "        s (str): date and time in Apache time format\n",
    "    Returns:\n",
    "        datetime: datetime object (ignore timezone for now)\n",
    "    \"\"\"\n",
    "    return datetime.datetime(int(s[7:11]),\n",
    "                             month_map[s[3:6]],\n",
    "                             int(s[0:2]),  # Day\n",
    "                             int(s[12:14]),\n",
    "                             int(s[15:17]),\n",
    "                             int(s[18:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Each Log Line\n",
    "#### Using the CLF as defined above, we create a regular expression pattern to extract the nine fields of the log line using the Python regular expression search function. The function returns a pair consisting of a Row object and 1. If the log line fails to match the regular expression, the function returns a pair consisting of the log line string and 0. A '-' value in the content size field is cleaned up by substituting it with 0. The function converts the log line's date string into a Python datetime object using the given parse_apache_time function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "APACHE_ACCESS_LOG_PATTERN = '^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+)\\s*(\\S*)\" (\\d{3}) (\\S+)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseApacheLogLine(logline):\n",
    "    \"\"\" Parse a line in the Apache Common Log format\n",
    "    Args:\n",
    "        logline (str): a line of text in the Apache Common Log format\n",
    "    Returns:\n",
    "        tuple: either a dictionary containing the parts of the Apache Access Log and 1,\n",
    "               or the original invalid log line and 0\n",
    "    \"\"\"\n",
    "    match = re.search(APACHE_ACCESS_LOG_PATTERN, logline)\n",
    "    if match is None:\n",
    "        return (logline, 0)\n",
    "    size_field = match.group(9)\n",
    "    if size_field == '-':\n",
    "        size = 0\n",
    "    else:\n",
    "        size = match.group(9)\n",
    "    return (Row(\n",
    "        host          = match.group(1),\n",
    "        client_identd = match.group(2),\n",
    "        user_id       = match.group(3),\n",
    "        date_time     = parse_apache_time(match.group(4)),\n",
    "        method        = match.group(5),\n",
    "        endpoint      = match.group(6),\n",
    "        protocol      = match.group(7),\n",
    "        response_code = int(match.group(8)),\n",
    "        content_size  = int(size)\n",
    "    ), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration and Initial RDD Creation\n",
    "#### We are ready to specify the input log file and create an RDD containing the parsed log file data. The log file has already been downloaded for you.\n",
    "\n",
    "#### To create the primary RDD that we'll use in the rest of this assignment, we first load the text file using sc.textfile(logFile) to convert each line of the file into an element in an RDD.\n",
    "\n",
    "#### Next, we use map(parseApacheLogLine) to apply the parse function to each element (that is, a line from the log file) in the RDD and turn each line into a pair Row object.\n",
    "\n",
    "#### Finally, we cache the RDD in memory since we'll use it throughout this notebook.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLogs( log_file ):\n",
    "    \"\"\" Read and parse log file \"\"\"\n",
    "    parsed_logs = (sc\n",
    "                   .textFile( log_file )\n",
    "                   .map(parseApacheLogLine)\n",
    "                   .cache())\n",
    "\n",
    "    access_logs = (parsed_logs\n",
    "                   .filter(lambda s: s[1] == 1)\n",
    "                   .map(lambda s: s[0])\n",
    "                   .cache())\n",
    "\n",
    "    failed_logs = (parsed_logs\n",
    "                   .filter(lambda s: s[1] == 0)\n",
    "                   .map(lambda s: s[0]))\n",
    "    failed_logs_count = failed_logs.count()\n",
    "    if failed_logs_count > 0:\n",
    "        print( 'Number of invalid logline: %d' % failed_logs.count())\n",
    "        for line in failed_logs.take(20):\n",
    "            print( 'Invalid logline: %s' % line)\n",
    "\n",
    "    print('Read %d lines, successfully parsed %d lines, failed to parse %d lines' % (parsed_logs.count(), access_logs.count(), failed_logs.count()))\n",
    "    return parsed_logs, access_logs, failed_logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 16 lines, successfully parsed 16 lines, failed to parse 0 lines\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "logFile = 'data/usask_access_sanity_check_log'\n",
    "parsed_logs, access_logs, failed_logs = parseLogs(logFile)\n",
    "#failed_logs.take(5)\n",
    "#access_logs.take(10)\n",
    "print(access_logs.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "Top Ten failed URLs: [('/images/logo.gif', 9)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Count URLs in erro (not code 200)\n",
    "\n",
    "not200 = access_logs.map(lambda log: log).filter(lambda log: log.response_code != 200)\n",
    "print(not200.count())\n",
    "endpointCountPairTuple = not200.map(lambda log: (log.endpoint, 1))\n",
    "\n",
    "endpointSum = endpointCountPairTuple.reduceByKey(add)\n",
    "#print(endpointSum.collect())\n",
    "topTenErrURLs = endpointSum.takeOrdered(10)\n",
    "print('Top Ten failed URLs: %s' % topTenErrURLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99988\n",
      "Average Content size : 7396 \n",
      "(count: 99988, mean: 7396.535384246124, stdev: 275491.2270841608, max: 30193824.0, min: 0.0)\n"
     ]
    }
   ],
   "source": [
    "content_sizes = access_logs.map(lambda log: log.content_size).cache()\n",
    "\n",
    "print(content_sizes.count())\n",
    "avg_content_size = content_sizes.reduce(lambda a, b : a + b) / content_sizes.count()\n",
    "print('Average Content size : %d ' % avg_content_size )\n",
    "#print(type(content_sizes.min()) )\n",
    "#print('Minimum size is : %d' % content_sizes.min())\n",
    "#print('Minimum size is : %d' % content_sizes.max())\n",
    "print(content_sizes.stats())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['skul2.usask.ca', 'bell.usask.ca', '142.99.48.34', 'villi.usask.ca', 'chemeng03']\n",
      "Unique hosts: 10\n"
     ]
    }
   ],
   "source": [
    "# Number of unique Hosts\n",
    "hosts = access_logs.map(lambda log: log.host)\n",
    "print(hosts.take(5))\n",
    "uniqueHosts = hosts.distinct()\n",
    "uniqueHostCount = uniqueHosts.count()\n",
    "print( 'Unique hosts: %d' % uniqueHostCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month Stats\n",
      "(count: 16, mean: 6.0, stdev: 0.0, max: 6.0, min: 6.0)\n",
      "Day Stats\n",
      "(count: 16, mean: 15.0, stdev: 0.0, max: 15.0, min: 15.0)\n",
      "Hour Stats\n",
      "(count: 16, mean: 13.8125, stdev: 0.3903123748999, max: 14.0, min: 13.0)\n",
      "Second Stats\n",
      "(count: 16, mean: 28.0, stdev: 16.725728683677733, max: 50.0, min: 3.0)\n"
     ]
    }
   ],
   "source": [
    "dayTimeStatsMonth = access_logs.map(lambda log: float(log.date_time.month))\n",
    "dayTimeStatsDay = access_logs.map(lambda log: float(log.date_time.day))\n",
    "dayTimeStatsHour = access_logs.map(lambda log: float(log.date_time.hour))\n",
    "dayTimeStatsSecond = access_logs.map(lambda log: float(log.date_time.second))\n",
    "print(\"Month Stats\")\n",
    "print(dayTimeStatsMonth.stats())\n",
    "print(\"Day Stats\")\n",
    "print(dayTimeStatsDay.stats())\n",
    "print(\"Hour Stats\")\n",
    "print(dayTimeStatsHour.stats())\n",
    "print(\"Second Stats\")\n",
    "print(dayTimeStatsSecond.stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique hosts per day: [(15, 10)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "dayToHostPairTuple = access_logs.map(lambda log: ((log.date_time.day, log.host), 1))\n",
    "\n",
    "#\n",
    "#  shape : ((k1, k2), 1)\n",
    "\n",
    "dayGroupedHosts = dayToHostPairTuple.reduceByKey(lambda a, b: a + b)\n",
    "#  shape : ((k1, k2), total)\n",
    "\n",
    "dayHostCount = dayGroupedHosts.map(lambda x: (x[0][0], 1))\n",
    "#  shape : (k, v)\n",
    "\n",
    "dailyHosts = dayHostCount.reduceByKey(lambda a, b: a + b).sortByKey().cache()\n",
    "             \n",
    "dailyHostsList = dailyHosts.takeOrdered(30)\n",
    "print ('Unique hosts per day: %s' % dailyHostsList)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+-------------------+--------------------+--------------------+------+--------+-------------+-------+\n",
      "|client_identd|content_size|          date_time|            endpoint|                host|method|protocol|response_code|user_id|\n",
      "+-------------+------------+-------------------+--------------------+--------------------+------+--------+-------------+-------+\n",
      "|            -|        2360|1995-06-15 13:56:59|/~macphed/finite/...| geif15.insa-lyon.fr|   GET|        |          200|      -|\n",
      "|            -|        2190|1995-06-15 13:57:08|/~macphed/finite/...| geif15.insa-lyon.fr|   GET|        |          200|      -|\n",
      "|            -|        2146|1995-06-15 13:57:09|/~lowey/unprofess...|anonymous.chevron...|   GET|        |          200|      -|\n",
      "|            -|        4210|1995-06-15 13:57:12|/~scottp/freewww....|  ds7001-1.gnofn.org|   GET|        |          200|      -|\n",
      "|            -|       38469|1995-06-15 13:57:17|/~lowey/kev_dino.gif|anonymous.chevron...|   GET|        |          200|      -|\n",
      "|            -|        1563|1995-06-15 13:57:39|/~macphed/finite/...| geif15.insa-lyon.fr|   GET|        |          200|      -|\n",
      "|            -|        1090|1995-06-15 13:57:48|/~ladd/uminnesota...|toby.dot.state.mn.us|   GET|        |          200|      -|\n",
      "|            -|        2375|1995-06-15 13:58:00|/~macphed/finite/...| geif15.insa-lyon.fr|   GET|        |          200|      -|\n",
      "|            -|        1716|1995-06-15 13:58:08|/dcs/courses/cai/...|anther.mip.berkel...|   GET|HTTP/1.0|          200|      -|\n",
      "|            -|        1090|1995-06-15 13:58:16|/~ladd/uminnesota...|toby.dot.state.mn.us|   GET|        |          200|      -|\n",
      "|            -|        1640|1995-06-15 13:58:19| /images/logo_32.gif|anther.mip.berkel...|   GET|HTTP/1.0|          200|      -|\n",
      "|            -|           0|1995-06-15 13:58:19|          /education|     edu242.usask.ca|   GET|HTTP/1.0|          302|      -|\n",
      "|            -|         180|1995-06-15 13:58:23|/cgi-bin/hytelnet...|ccshst01.cs.uogue...|   GET|HTTP/1.0|          200|      -|\n",
      "|            -|        8337|1995-06-15 13:58:24|         /education/|     edu242.usask.ca|   GET|HTTP/1.0|          200|      -|\n",
      "|            -|        8337|1995-06-15 13:58:24|         /education/|     edu242.usask.ca|   GET|HTTP/1.0|          200|      -|\n",
      "|            -|         197|1995-06-15 13:58:29|/images/right_32.gif|anther.mip.berkel...|   GET|HTTP/1.0|          200|      -|\n",
      "|            -|           0|1995-06-15 13:58:30|          /education|     edu242.usask.ca|   GET|HTTP/1.0|          302|      -|\n",
      "|            -|         214|1995-06-15 13:58:31|/images/question_...|anther.mip.berkel...|   GET|HTTP/1.0|          200|      -|\n",
      "|            -|        1453|1995-06-15 13:58:31|/dcs/courses/cai/...|anther.mip.berkel...|   GET|HTTP/1.0|          200|      -|\n",
      "|            -|         149|1995-06-15 13:58:34|/images/letter_32...|anther.mip.berkel...|   GET|HTTP/1.0|          200|      -|\n",
      "+-------------+------------+-------------------+--------------------+--------------------+------+--------+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "access_logs.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
